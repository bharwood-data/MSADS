{"cells":[{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":1079,"status":"ok","timestamp":1606946029947,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"xBV1zzVFUb2v"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from wordcloud import WordCloud\n","from pandas import DataFrame\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.svm import LinearSVC\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":81791,"status":"ok","timestamp":1606946110663,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"K7P4t_WuUup0"},"outputs":[],"source":["file = pd.read_csv('/content/drive/My Drive/IST 736 Project/Final Table.csv')\n","\n","\n","\n","#file = pd.read_csv('/content/drive/MyDrive/IST 736 Project/Twitter User Search.csv')\n","\n","#file_b = pd.read_csv('/content/drive/MyDrive/IST 736 Project/Twitter Topic.csv')"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":81787,"status":"ok","timestamp":1606946110664,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"NMPkCDO_8hVb"},"outputs":[],"source":["np.unique(file['Party'])\n","\n","file_b = file[file['Party'] == 'No-Detail']\n","file = file[file['Party'] != 'No-Detail']"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83209,"status":"ok","timestamp":1606946112089,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"Ll1UFI5gUusz","outputId":"7f05d066-5a2d-42ce-ad5d-a95d8c9867b6"},"outputs":[{"data":{"text/plain":["array(['CV (Binary)', 'CV (Not Binary)', 'TF_IDF'], dtype=object)"]},"execution_count":27,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["filea = file.drop('Unnamed: 0', axis = 1)\n","filea = filea.drop('Label', axis = 1)\n","d_cv = filea[filea['Vectorizer'] == \"CV (Not Binary)\"]\n","d_cv2 = filea[filea['Vectorizer'] == \"CV (Binary)\"]\n","d_tf = filea[filea['Vectorizer'] == \"TF_IDF\"]\n","np.unique(filea.Vectorizer)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":83205,"status":"ok","timestamp":1606946112090,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"GjnXg1hdWwOp"},"outputs":[],"source":["d_cv = d_cv.drop('Vectorizer', axis = 1)\n","d_cv2 = d_cv2.drop('Vectorizer', axis = 1)\n","d_tf = d_tf.drop('Vectorizer', axis = 1)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":83609,"status":"ok","timestamp":1606946112496,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"ZZMqsh2OXuv-"},"outputs":[],"source":["def model_run(dataset_input, model_input, model_label, n_fold_cv_input, vect_label):\n","    Train_PCT_Sent = []\n","    Test_PCT_Sent = []\n","    \n","    for i in range(0,n_fold_cv_input):\n","        ## Create the testing set - grab a sample from the training set. \n","        ## Be careful. Notice that right now, our train set is sorted by label.\n","        ## If your train set is large enough, you can take a random sample.\n","        from sklearn.model_selection import train_test_split\n","\n","        TrainDF, TestDF = train_test_split(dataset_input, test_size=0.4)\n","        #TrainDFD, TestDFB = train_test_split(d_cv_desc, test_size=0.3)\n","\n","\n","        ##-----------------------------------------------------------------\n","        ##\n","        ## Now we have a training set and a testing set. \n","        #print(\"The training set is:\")\n","        #print(TrainDF)\n","        #print(\"The testing set is:\")\n","        #print(TestDF)\n","\n","        ## IMPORTANT - !!!!!!! SEPERATE LABELS !!!!!!\n","        ## Save labels\n","        TestLabels=TestDF['Party']\n","        #print(TestLabels)\n","        ## remove labels\n","        TestDF = TestDF.drop([\"Party\"], axis=1)\n","        #print(TestDF)\n","        ## When you look up this model, you learn that it wants the \n","        ## DF seperate from the labels\n","        TrainDF_nolabels=TrainDF.drop([\"Party\"], axis=1)\n","        #print(TrainDF_nolabels)\n","        TrainLabels=TrainDF[\"Party\"]\n","        #print(TrainLabels)\n","\n","        MyModelNB= model_input\n","\n","        MyModelNB.fit(TrainDF_nolabels, TrainLabels)\n","\n","        Prediction = MyModelNB.predict(TrainDF_nolabels)\n","        #print(\"The prediction from NB is:\")\n","        #print(Prediction)\n","        #print(\"The actual labels are:\")\n","        #print(TestLabels)\n","        ## confusion matrix\n","        from sklearn.metrics import confusion_matrix\n","        ## The confusion matrix is square and is labels X labels\n","        ## We ahve two labels, so ours will be 2X2\n","        #The matrix shows\n","        ## rows are the true labels\n","        ## columns are predicted\n","        ## it is al[habetical\n","        ## The numbers are how many \n","        cnf_matrix_train = confusion_matrix(TrainLabels, Prediction)\n","        print(\"The confusion matrix for training is:\")\n","        print(cnf_matrix_train, '\\n')\n","        ### prediction probabilities\n","        ## columns are the labels in alphabetical order\n","        ## The decinal in the matrix are the prob of being\n","        ## that label\n","        #print(np.round(MyModelNB.predict_proba(TestDF),4))\n","\n","        pct_correct = (100*((cnf_matrix_train[0,0]+cnf_matrix_train[1,1])/\n","                 (cnf_matrix_train[0,0] + cnf_matrix_train[0,1] + cnf_matrix_train[1,0] + cnf_matrix_train[1,1]))).round(3)\n","\n","\n","\n","        print(\"Percent Correct for Train Data: \", pct_correct)\n","\n","        Train_PCT_Sent.append(pct_correct)\n","\n","\n","        Prediction = MyModelNB.predict(TestDF)\n","        #print(\"The prediction from NB is:\")\n","        #print(Prediction)\n","        #print(\"The actual labels are:\")\n","        #print(TestLabels)\n","        ## confusion matrix\n","        from sklearn.metrics import confusion_matrix\n","        ## The confusion matrix is square and is labels X labels\n","        ## We ahve two labels, so ours will be 2X2\n","        #The matrix shows\n","        ## rows are the true labels\n","        ## columns are predicted\n","        ## it is al[habetical\n","        ## The numbers are how many \n","        cnf_matrix_test = confusion_matrix(TestLabels, Prediction)\n","        print(\"The confusion matrix for testing is:\")\n","        print(cnf_matrix_test, '\\n')\n","        ### prediction probabilities\n","        ## columns are the labels in alphabetical order\n","        ## The decinal in the matrix are the prob of being\n","        ## that label\n","        #print(np.round(MyModelNB.predict_proba(TestDF),4))\n","\n","        pct_correct = (100*((cnf_matrix_test[0,0]+cnf_matrix_test[1,1])/\n","                 (cnf_matrix_test[0,0] + cnf_matrix_test[0,1] + cnf_matrix_test[1,0] + cnf_matrix_test[1,1]))).round(3)\n","\n","\n","        print(\"Percent Correct for Test Data: \", pct_correct)\n","\n","        Test_PCT_Sent.append(pct_correct)   \n","        \n","    Comparison = DataFrame(list(zip(Train_PCT_Sent,Test_PCT_Sent)), columns = ['Train', 'Test'])\n","\n","    #print(n_fold_cv_input, ' Fold CV Average Train Data Classification for Sentiment: ', round(Comparison['Train'].mean(),2), \"Percent\")\n","    #print(n_fold_cv_input, ' Fold CV Average Test Data Classification: for Sentiment:', round(Comparison['Test'].mean(),2), \"Percent\")\n","\n","    Comparison1 = Comparison\n","    \n","    Comparison = Comparison\n","\n","    Comparison.loc['Mean'] = Comparison.mean()\n","\n","    Comparison.reset_index(inplace = True)\n","\n","    Comparison.columns = ['Model', 'Train', 'Test']\n","    \n","    #Comparison.columns = ['Model', 'Train', 'Test']\n","\n","    Comparison_mean = (Comparison[Comparison['Model']== \"Mean\"]).round(2)\n","\n","    Comparison_mean['Model'] = model_label\n","\n","    Comparison_mean['Vectorizer'] = vect_label\n","\n","    Comparison_mean = Comparison_mean[['Model', 'Vectorizer', 'Train', 'Test']]\n","    \n","    Comparison1.loc['Mean'] = Comparison1.mean()\n","    \n","    Comparison1['Model'] = model_label\n","\n","    Comparison1['Vectorizer'] = vect_label\n","\n","    Comparison1 = Comparison1[['Model', 'Vectorizer', 'Train', 'Test']]\n","\n","    Comparison1.reset_index(inplace = True)\n","\n","    return(Comparison_mean)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":83605,"status":"ok","timestamp":1606946112496,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"RuIm9aUkp-4m"},"outputs":[],"source":["n_fold_validation = 1"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":86684,"status":"ok","timestamp":1606946115579,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"D9Fqgy51YG7b","outputId":"702a68b8-bb22-44d2-9fe5-52742e6d725c"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1172  165]\n"," [  22 1341]] \n","\n","Percent Correct for Train Data:  93.074\n","The confusion matrix for testing is:\n","[[670 243]\n"," [107 780]] \n","\n","Percent Correct for Test Data:  80.556\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eBernoulliNB\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer (Bindary)\u003c/td\u003e\n","      \u003ctd\u003e93.07\u003c/td\u003e\n","      \u003ctd\u003e80.56\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["         Model                 Vectorizer  Train   Test\n","1  BernoulliNB  CountVectorizer (Bindary)  93.07  80.56"]},"execution_count":31,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["\n","\n","model_run(d_cv2, BernoulliNB(),\"BernoulliNB\", n_fold_validation, \"CountVectorizer (Bindary)\")\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":87020,"status":"ok","timestamp":1606946115921,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"6upoP6_BYHRN","outputId":"02cb22c9-2f3b-44ae-c4ef-4206764a6988"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1333   18]\n"," [  99 1250]] \n","\n","Percent Correct for Train Data:  95.667\n","The confusion matrix for testing is:\n","[[794 105]\n"," [219 682]] \n","\n","Percent Correct for Test Data:  82.0\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eMultinomial NB\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e95.67\u003c/td\u003e\n","      \u003ctd\u003e82.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["            Model       Vectorizer  Train  Test\n","1  Multinomial NB  CountVectorizer  95.67  82.0"]},"execution_count":32,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model_run(d_cv, MultinomialNB(), \"Multinomial NB\", n_fold_validation, \"CountVectorizer\")\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":88102,"status":"ok","timestamp":1606946117007,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"OazdLjcGYHaK","outputId":"3032b42c-238a-418b-c449-c73b4a1c81e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1369    9]\n"," [ 178 1144]] \n","\n","Percent Correct for Train Data:  93.074\n","The confusion matrix for testing is:\n","[[801  71]\n"," [374 554]] \n","\n","Percent Correct for Test Data:  75.278\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eMultinomial NB\u003c/td\u003e\n","      \u003ctd\u003eTF-IDF\u003c/td\u003e\n","      \u003ctd\u003e93.07\u003c/td\u003e\n","      \u003ctd\u003e75.28\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["            Model Vectorizer  Train   Test\n","1  Multinomial NB     TF-IDF  93.07  75.28"]},"execution_count":33,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model_run(d_tf, MultinomialNB(), \"Multinomial NB\", n_fold_validation, \"TF-IDF\")\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":88897,"status":"ok","timestamp":1606946117809,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"nYZK9lP6YHkM","outputId":"6d3b40d3-a5b9-46fe-9de3-d84d313e40a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1299   44]\n"," [  74 1283]] \n","\n","Percent Correct for Train Data:  95.63\n","The confusion matrix for testing is:\n","[[793 114]\n"," [199 694]] \n","\n","Percent Correct for Test Data:  82.611\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eMultinomial NB (Binary)\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer (Binary)\u003c/td\u003e\n","      \u003ctd\u003e95.63\u003c/td\u003e\n","      \u003ctd\u003e82.61\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["                     Model                Vectorizer  Train   Test\n","1  Multinomial NB (Binary)  CountVectorizer (Binary)  95.63  82.61"]},"execution_count":34,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model_run(d_cv2, MultinomialNB(), \"Multinomial NB (Binary)\", n_fold_validation, \"CountVectorizer (Binary)\")\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":441408,"status":"ok","timestamp":1606946470326,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"bPOfJPNtYHuD","outputId":"9dcbc14c-ffa6-4fee-c2e6-9e9849530f5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1274   65]\n"," [  46 1315]] \n","\n","Percent Correct for Train Data:  95.889\n","The confusion matrix for testing is:\n","[[665 246]\n"," [190 699]] \n","\n","Percent Correct for Test Data:  75.778\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eSVC (Linear)\u003c/td\u003e\n","      \u003ctd\u003eTF-IDF\u003c/td\u003e\n","      \u003ctd\u003e95.89\u003c/td\u003e\n","      \u003ctd\u003e75.78\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["          Model Vectorizer  Train   Test\n","1  SVC (Linear)     TF-IDF  95.89  75.78"]},"execution_count":35,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model_run(d_tf, SVC(C=1, kernel = \"linear\", gamma = 'auto'), \"SVC (Linear)\", n_fold_validation, \"TF-IDF\")\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":856391,"status":"ok","timestamp":1606946885314,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"-8LsOkqKYH3L","outputId":"397a4a50-7d35-464d-aec9-0db9ad570c09"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[   0 1346]\n"," [   0 1354]] \n","\n","Percent Correct for Train Data:  50.148\n","The confusion matrix for testing is:\n","[[  0 904]\n"," [  0 896]] \n","\n","Percent Correct for Test Data:  49.778\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eSVC (Poly)\u003c/td\u003e\n","      \u003ctd\u003eTF-IDF\u003c/td\u003e\n","      \u003ctd\u003e50.15\u003c/td\u003e\n","      \u003ctd\u003e49.78\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["        Model Vectorizer  Train   Test\n","1  SVC (Poly)     TF-IDF  50.15  49.78"]},"execution_count":36,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model_run(d_tf, SVC(C=1, kernel = \"poly\", gamma = 'auto'), \"SVC (Poly)\", n_fold_validation, \"TF-IDF\")\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":1274758,"status":"ok","timestamp":1606947303688,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"Jv1MIQOUYH-6","outputId":"2f30cc4d-d27c-4a94-ba86-5d7fe65c30cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[   0 1333]\n"," [   0 1367]] \n","\n","Percent Correct for Train Data:  50.63\n","The confusion matrix for testing is:\n","[[  0 917]\n"," [  0 883]] \n","\n","Percent Correct for Test Data:  49.056\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eSVC (Radial)\u003c/td\u003e\n","      \u003ctd\u003eTF-IDF\u003c/td\u003e\n","      \u003ctd\u003e50.63\u003c/td\u003e\n","      \u003ctd\u003e49.06\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["          Model Vectorizer  Train   Test\n","1  SVC (Radial)     TF-IDF  50.63  49.06"]},"execution_count":37,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model_run(d_tf, SVC(C=1, kernel = \"rbf\", gamma = 'auto'), \"SVC (Radial)\", n_fold_validation, \"TF-IDF\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UaEm5FJtYIGI"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1339   23]\n"," [   1 1337]] \n","\n","Percent Correct for Train Data:  99.111\n","The confusion matrix for testing is:\n","[[654 234]\n"," [165 747]] \n","\n","Percent Correct for Test Data:  77.833\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eSVC (Linear)\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e99.11\u003c/td\u003e\n","      \u003ctd\u003e77.83\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["          Model       Vectorizer  Train   Test\n","1  SVC (Linear)  CountVectorizer  99.11  77.83"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model_run(d_cv, SVC(C=1, kernel = \"linear\", gamma = 'auto'), \"SVC (Linear)\", n_fold_validation, \"CountVectorizer\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"X86ksKW_YINg"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[   0 1346]\n"," [   0 1354]] \n","\n","Percent Correct for Train Data:  50.148\n","The confusion matrix for testing is:\n","[[  0 904]\n"," [  0 896]] \n","\n","Percent Correct for Test Data:  49.778\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eSVC (Poly)\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e50.15\u003c/td\u003e\n","      \u003ctd\u003e49.78\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["        Model       Vectorizer  Train   Test\n","1  SVC (Poly)  CountVectorizer  50.15  49.78"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model_run(d_cv, SVC(C=1, kernel = \"poly\", gamma = 'auto'), \"SVC (Poly)\", n_fold_validation, \"CountVectorizer\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vo_cxZmhYVUW"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[   0 1343]\n"," [   0 1357]] \n","\n","Percent Correct for Train Data:  50.259\n","The confusion matrix for testing is:\n","[[  0 907]\n"," [  0 893]] \n","\n","Percent Correct for Test Data:  49.611\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eSVC (Radial)\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e50.26\u003c/td\u003e\n","      \u003ctd\u003e49.61\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["          Model       Vectorizer  Train   Test\n","1  SVC (Radial)  CountVectorizer  50.26  49.61"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model_run(d_cv, SVC(C=1, kernel = \"rbf\", gamma = 'auto'), \"SVC (Radial)\", n_fold_validation, \"CountVectorizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yoVMeoatYIVl"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[ 937  399]\n"," [ 168 1196]] \n","\n","Percent Correct for Train Data:  79.0\n","The confusion matrix for testing is:\n","[[493 421]\n"," [171 715]] \n","\n","Percent Correct for Test Data:  67.111\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eDecision Tree\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e79.0\u003c/td\u003e\n","      \u003ctd\u003e67.11\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["           Model       Vectorizer  Train   Test\n","1  Decision Tree  CountVectorizer   79.0  67.11"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model_run(d_cv, DecisionTreeClassifier(min_samples_leaf = 5, max_depth = 50), \"Decision Tree\", n_fold_validation, \"CountVectorizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3-d0brt-uakm"},"outputs":[{"name":"stdout","output_type":"stream","text":["The confusion matrix for training is:\n","[[1314   20]\n"," [   0 1366]] \n","\n","Percent Correct for Train Data:  99.259\n","The confusion matrix for testing is:\n","[[710 206]\n"," [179 705]] \n","\n","Percent Correct for Test Data:  78.611\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eRF\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e99.26\u003c/td\u003e\n","      \u003ctd\u003e78.61\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["  Model       Vectorizer  Train   Test\n","1    RF  CountVectorizer  99.26  78.61"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model_run(d_cv, RandomForestClassifier(n_estimators = 500, n_jobs= -1), \"RF\", n_fold_validation, \"CountVectorizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TB2PD7KcUuvp"},"outputs":[],"source":["\n","def model_run_v2(dataset_input, model_input, model_label, n_fold_cv_input, vect_label):\n","    Train_PCT_Sent = []\n","    Test_PCT_Sent = []\n","    \n","    for i in range(0,n_fold_cv_input):\n","        ## Create the testing set - grab a sample from the training set. \n","        ## Be careful. Notice that right now, our train set is sorted by label.\n","        ## If your train set is large enough, you can take a random sample.\n","        from sklearn.model_selection import train_test_split\n","\n","        TrainDF, TestDF = train_test_split(dataset_input, test_size=0.4)\n","        #TrainDFD, TestDFB = train_test_split(d_cv_desc, test_size=0.3)\n","\n","\n","        ##-----------------------------------------------------------------\n","        ##\n","        ## Now we have a training set and a testing set. \n","        #print(\"The training set is:\")\n","        #print(TrainDF)\n","        #print(\"The testing set is:\")\n","        #print(TestDF)\n","\n","        ## IMPORTANT - !!!!!!! SEPERATE LABELS !!!!!!\n","        ## Save labels\n","        TestLabels=TestDF['Party']\n","        #print(TestLabels)\n","        ## remove labels\n","        TestDF = TestDF.drop([\"Party\"], axis=1)\n","        #print(TestDF)\n","        ## When you look up this model, you learn that it wants the \n","        ## DF seperate from the labels\n","        TrainDF_nolabels=TrainDF.drop([\"Party\"], axis=1)\n","        #print(TrainDF_nolabels)\n","        TrainLabels=TrainDF[\"Party\"]\n","        #print(TrainLabels)\n","\n","        MyModelNB= model_input\n","\n","        MyModelNB.fit(TrainDF_nolabels, TrainLabels)\n","\n","        Prediction = MyModelNB.predict(TrainDF_nolabels)\n","        #print(\"The prediction from NB is:\")\n","        #print(Prediction)\n","        #print(\"The actual labels are:\")\n","        #print(TestLabels)\n","        ## confusion matrix\n","        from sklearn.metrics import confusion_matrix\n","        ## The confusion matrix is square and is labels X labels\n","        ## We ahve two labels, so ours will be 2X2\n","        #The matrix shows\n","        ## rows are the true labels\n","        ## columns are predicted\n","        ## it is al[habetical\n","        ## The numbers are how many \n","        cnf_matrix_train = confusion_matrix(TrainLabels, Prediction)\n","        #print(\"The confusion matrix for training is:\")\n","        #print(cnf_matrix_train, '\\n')\n","        ### prediction probabilities\n","        ## columns are the labels in alphabetical order\n","        ## The decinal in the matrix are the prob of being\n","        ## that label\n","        #print(np.round(MyModelNB.predict_proba(TestDF),4))\n","\n","        pct_correct = (100*((cnf_matrix_train[0,0]+cnf_matrix_train[1,1])/\n","                 (cnf_matrix_train[0,0] + cnf_matrix_train[0,1] + cnf_matrix_train[1,0] + cnf_matrix_train[1,1]))).round(3)\n","\n","\n","\n","        #print(\"Percent Correct for Train Data: \", pct_correct)\n","\n","        Train_PCT_Sent.append(pct_correct)\n","\n","\n","        Prediction = MyModelNB.predict(TestDF)\n","        #print(\"The prediction from NB is:\")\n","        #print(Prediction)\n","        #print(\"The actual labels are:\")\n","        #print(TestLabels)\n","        ## confusion matrix\n","        from sklearn.metrics import confusion_matrix\n","        ## The confusion matrix is square and is labels X labels\n","        ## We ahve two labels, so ours will be 2X2\n","        #The matrix shows\n","        ## rows are the true labels\n","        ## columns are predicted\n","        ## it is al[habetical\n","        ## The numbers are how many \n","        cnf_matrix_test = confusion_matrix(TestLabels, Prediction)\n","        #print(\"The confusion matrix for testing is:\")\n","        #print(cnf_matrix_test, '\\n')\n","        ### prediction probabilities\n","        ## columns are the labels in alphabetical order\n","        ## The decinal in the matrix are the prob of being\n","        ## that label\n","        #print(np.round(MyModelNB.predict_proba(TestDF),4))\n","\n","        pct_correct = (100*((cnf_matrix_test[0,0]+cnf_matrix_test[1,1])/\n","                 (cnf_matrix_test[0,0] + cnf_matrix_test[0,1] + cnf_matrix_test[1,0] + cnf_matrix_test[1,1]))).round(3)\n","\n","\n","        #print(\"Percent Correct for Test Data: \"\n","        #      , pct_correct)\n","\n","        Test_PCT_Sent.append(pct_correct)   \n","        \n","    Comparison = DataFrame(list(zip(Train_PCT_Sent,Test_PCT_Sent)), columns = ['Train', 'Test'])\n","\n","    #print(n_fold_cv_input, ' Fold CV Average Train Data Classification for Sentiment: ', round(Comparison['Train'].mean(),2), \"Percent\")\n","    #print(n_fold_cv_input, ' Fold CV Average Test Data Classification: for Sentiment:', round(Comparison['Test'].mean(),2), \"Percent\")\n","\n","    Comparison1 = Comparison\n","    \n","    Comparison = Comparison\n","\n","    Comparison.loc['Mean'] = Comparison.mean()\n","\n","    Comparison.reset_index(inplace = True)\n","\n","    Comparison.columns = ['Model', 'Train', 'Test']\n","    \n","    #Comparison.columns = ['Model', 'Train', 'Test']\n","\n","    Comparison_mean = (Comparison[Comparison['Model']== \"Mean\"]).round(2)\n","\n","    Comparison_mean['Model'] = model_label\n","\n","    Comparison_mean['Vectorizer'] = vect_label\n","\n","    Comparison_mean = Comparison_mean[['Model', 'Vectorizer', 'Train', 'Test']]\n","    \n","    Comparison1.loc['Mean'] = Comparison1.mean()\n","    \n","    Comparison1['Model'] = model_label\n","\n","    Comparison1['Vectorizer'] = vect_label\n","\n","    Comparison1 = Comparison1[['Model', 'Vectorizer', 'Train', 'Test']]\n","\n","    Comparison1.reset_index(inplace = True)\n","\n","    return(Comparison_mean)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":5925397,"status":"ok","timestamp":1606946026668,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"MUBhLGlFUuyb","outputId":"a9218827-d8c9-4d39-fd58-c5a36d89a7a7"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eModel\u003c/th\u003e\n","      \u003cth\u003eVectorizer\u003c/th\u003e\n","      \u003cth\u003eTrain\u003c/th\u003e\n","      \u003cth\u003eTest\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eMultinomial NB\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e95.63\u003c/td\u003e\n","      \u003ctd\u003e81.81\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eMultinomial NB (Binary)\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer (Binary)\u003c/td\u003e\n","      \u003ctd\u003e95.80\u003c/td\u003e\n","      \u003ctd\u003e80.60\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eBernoulliNB\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer (Bindary)\u003c/td\u003e\n","      \u003ctd\u003e93.40\u003c/td\u003e\n","      \u003ctd\u003e80.59\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eSVC (Linear)\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e99.00\u003c/td\u003e\n","      \u003ctd\u003e78.42\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eRF\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e99.28\u003c/td\u003e\n","      \u003ctd\u003e77.63\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e5\u003c/th\u003e\n","      \u003ctd\u003eSVC (Linear)\u003c/td\u003e\n","      \u003ctd\u003eTF-IDF\u003c/td\u003e\n","      \u003ctd\u003e95.90\u003c/td\u003e\n","      \u003ctd\u003e76.42\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e6\u003c/th\u003e\n","      \u003ctd\u003eMultinomial NB\u003c/td\u003e\n","      \u003ctd\u003eTF-IDF\u003c/td\u003e\n","      \u003ctd\u003e92.93\u003c/td\u003e\n","      \u003ctd\u003e74.98\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7\u003c/th\u003e\n","      \u003ctd\u003eDecision Tree\u003c/td\u003e\n","      \u003ctd\u003eCountVectorizer\u003c/td\u003e\n","      \u003ctd\u003e78.45\u003c/td\u003e\n","      \u003ctd\u003e66.97\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["                     Model                 Vectorizer  Train   Test\n","0           Multinomial NB            CountVectorizer  95.63  81.81\n","1  Multinomial NB (Binary)   CountVectorizer (Binary)  95.80  80.60\n","2              BernoulliNB  CountVectorizer (Bindary)  93.40  80.59\n","3             SVC (Linear)            CountVectorizer  99.00  78.42\n","4                       RF            CountVectorizer  99.28  77.63\n","5             SVC (Linear)                     TF-IDF  95.90  76.42\n","6           Multinomial NB                     TF-IDF  92.93  74.98\n","7            Decision Tree            CountVectorizer  78.45  66.97"]},"execution_count":21,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["n_fold_validation = 5\n","\n","d1 = model_run_v2(d_cv2, BernoulliNB(),\"BernoulliNB\", n_fold_validation, \"CountVectorizer (Bindary)\")\n","d2 = model_run_v2(d_cv, MultinomialNB(), \"Multinomial NB\", n_fold_validation, \"CountVectorizer\")\n","d3 = model_run_v2(d_tf, MultinomialNB(), \"Multinomial NB\", n_fold_validation, \"TF-IDF\")\n","d4 = model_run_v2(d_cv2, MultinomialNB(), \"Multinomial NB (Binary)\", n_fold_validation, \"CountVectorizer (Binary)\")\n","d5 = model_run_v2(d_tf, SVC(C=1, kernel = \"linear\", gamma = 'auto'), \"SVC (Linear)\", n_fold_validation, \"TF-IDF\")\n","#d6 = model_run_v2(d_tf, SVC(C=1, kernel = \"poly\", gamma = 'auto'), \"SVC (Poly)\", n_fold_validation, \"TF-IDF\")\n","#d7 = model_run_v2(d_tf, SVC(C=1, kernel = \"rbf\", gamma = 'auto'), \"SVC (Radial)\", n_fold_validation, \"TF-IDF\")\n","d8 = model_run_v2(d_cv, SVC(C=1, kernel = \"linear\", gamma = 'auto'), \"SVC (Linear)\", n_fold_validation, \"CountVectorizer\")\n","#d9 = model_run_v2(d_cv, SVC(C=1, kernel = \"poly\", gamma = 'auto'), \"SVC (Poly)\", n_fold_validation, \"CountVectorizer\")\n","#d10 = model_run_v2(d_cv, SVC(C=1, kernel = \"rbf\", gamma = 'auto'), \"SVC (Radial)\", n_fold_validation, \"CountVectorizer\")\n","d11 = model_run_v2(d_cv, DecisionTreeClassifier(min_samples_leaf = 5, max_depth = 50), \"Decision Tree\", n_fold_validation, \"CountVectorizer\")\n","d12 = model_run_v2(d_cv, RandomForestClassifier(n_estimators = 500, n_jobs= -1), \"RF\", n_fold_validation, \"CountVectorizer\")\n","final = pd.concat([d1,\n","                   d2,\n","                   d3,\n","                   d4, \n","                   d5,\n","                   #d6,\n","                   #d7,\n","                   d8,\n","                   #d9, \n","                   #d10\n","                   d11,\n","                   d12\n","                   ], join = \"outer\").reset_index()\n","\n","final = final.drop('index', axis = 1)\n","\n","final = final.sort_values('Test', ascending = False).reset_index()\n","\n","final = final.drop('index', axis = 1)\n","\n","final"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5926642,"status":"ok","timestamp":1606946027915,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"zQr3k7W6Uu06"},"outputs":[],"source":["final.to_excel('/content/drive/MyDrive/IST 736 Project/Compare Models.xlsx')"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5927267,"status":"ok","timestamp":1606946028542,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"569Ei40WUu3j"},"outputs":[],"source":["final.to_csv('/content/drive/MyDrive/IST 736 Project/Compare Models.csv')"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5927267,"status":"ok","timestamp":1606946028543,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"D7Ji1lrYUu6K"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5927266,"status":"ok","timestamp":1606946028543,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"0F93VsC3Uu8-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5927265,"status":"ok","timestamp":1606946028544,"user":{"displayName":"Christopher Webster","photoUrl":"","userId":"00422067514763521290"},"user_tz":480},"id":"IFbw0GWPUu_S"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN58CgmhMY56vftrKDLyDQv","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1IaE2MFxa2tBebrVCM7vljdKLWC6s0prN","name":"Project 736 - model.ipynb","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}